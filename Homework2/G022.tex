
% RUN:
% pdflatex -output-directory=/Users/salvatorpes/Desktop/Aprendizagem/Homework2/trash /Users/salvatorpes/Desktop/Aprendizagem/Homework2/G022.tex

% ir a settings.json e adicionar:
% // According to the wiki, the string latex-workshop.latex.autoBuild.run has three possible values: never, onSave and onFileChange(default).
% "latex-workshop.latex.autoBuild.run": "never",

\documentclass{article}

\author{Pedro Curvo (ist1102716) $|$ Salvador Torpes (ist1102474)}

\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
% \usepackage[letterpaper,top=10mm,bottom=15mm,left=15mm,right=15mm,marginparwidth=1.75cm]{geometry}
% \usepackage[letterpaper,top=10mm,bottom=15mm,left=15mm,right=15mm,marginparwidth=1.75cm]{geometry}
\usepackage[letterpaper,margin=1in,marginparwidth=1.75cm]{geometry}
\usepackage{multicol}
\usepackage{biblatex}
\usepackage{colortbl}
\addbibresource{Bibliografia.bib}
\usepackage{graphicx}
% \graphicspath{{../Homework1/images/}}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{makecell}
\usepackage{url}
\usepackage{csquotes}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{textcomp}
\usepackage{pdflscape}
\usepackage{makeidx}
\usepackage{amsmath}
% \usepackage{tocbibind}
\providecommand{\tightlist}{\relax}
\usepackage{tocloft}
\renewcommand{\cftsecindent}{0em}
\renewcommand{\cftsubsecindent}{1em}
\renewcommand{\cftsecfont}{\bfseries}
\renewcommand{\cftsubsecfont}{\itshape}
\setlength{\cftsubsecnumwidth}{0em}

\usepackage[version=4]{mhchem}
\usepackage{hyperref} % Remove "pdftex" option here
\usepackage{float}
\usepackage{fancyhdr}
\usepackage{ragged2e}
\usepackage{xkeyval}
%\usepackage{minted}
%\usemintedstyle{manni}
\usepackage{listings}
\usepackage{amssymb}




\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{positioning, arrows.meta}
\usepackage{adjustbox}
\usepackage{sidecap}



% \usepackage[table,xcdraw]{xcolor}
\usepackage[LY1]{fontenc}
\usepackage{tikz-3dplot}
% \usepackage{pgfplots}
\usetikzlibrary{calc, 3d, arrows}
\usepackage{forest}




\usetikzlibrary{shapes.geometric, arrows}


\lstset{
    language=Python,
    basicstyle=\ttfamily,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{orange},
    numbers=left,
    numberstyle=\tiny,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    breaklines=true,
    frame=tb,
    framexleftmargin=2em,
    xleftmargin=2em,
}


%\usepackage{fontspec}

%\setmonofont{Fira Code}

\fancyhf{}
\cfoot{\thepage}
\fancyhf{} % Clear all header and footer fields
\renewcommand{\headrulewidth}{0pt} % Remove the header rule line
\cfoot{\thepage} % Set the page number in the center of the footer

\pagestyle{fancy} % Apply the fancy page style

\setlength\columnsep{20pt}

\renewcommand{\familydefault}{\sfdefault}

\newenvironment{Figure}
  {\par\medskip\noindent\minipage{\linewidth}}
  {\endminipage\par\medskip}

\makeatletter
\newenvironment{figurehere}
{\def\@captype{figure}}
{}
\makeatother

\hypersetup{
  colorlinks,
  linkcolor=blue,
  anchorcolor=black,
  citecolor=cyan,
  filecolor=cyan,
  menucolor=cyan,
  urlcolor=cyan,
  bookmarksopen=true,
  bookmarksnumbered=true
}

\makeindex


\title{\vspace{-6mm}\includegraphics[width=15mm,scale=2]{images/IST_Logo.png}\\ \vspace{5mm}
Machine Learning - Homework 2 \vspace{-5mm}}
\date{1st Term - 23/24}

\usepackage{sansmathfonts}
\usepackage[T1]{fontenc}
\usepackage[OT1]{fontenc}

\usepackage{listings}
\usepackage{xcolor}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}

\lstdefinestyle{mystyle}{
  backgroundcolor=\color{backcolour},   
  commentstyle=\color{codegray},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codegreen},
  keywordstyle=[2]{\color{orange}},
  keywords=[2]{plt.},
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,         
  breaklines=true,                 
  captionpos=b,                    
  keepspaces=true,                 
  numbers=left,                    
  numbersep=5pt,                  
  showspaces=false,                
  showstringspaces=false,
  showtabs=false,                  
  tabsize=2,
  frame=single,
  framesep=2pt,
  framerule=0pt,
  xleftmargin=2pt,
  xrightmargin=2pt,
  aboveskip=1em,
  belowskip=1em,
  abovecaptionskip=0.5em,
  belowcaptionskip=0.5em,
  caption=\lstname,
  captionpos=b,
  language=Python,
  morekeywords={as},
  deletekeywords={None},
  emph={self},
  emphstyle=\color{blue},
  escapeinside={(*@}{@*)},
  literate={+}{{\textcolor{blue}{+}}}1
       {*}{{\textcolor{blue}{*}}}1
       {-}{{\textcolor{blue}{-}}}1
       {/}{{\textcolor{blue}{/}}}1
       {=}{{\textcolor{blue}{=}}}1
       {>}{{\textcolor{blue}{>}}}1
       {<}{{\textcolor{blue}{<}}}1
       {==}{{\textcolor{blue}{==}}}2
       {!=}{{\textcolor{blue}{!=}}}2
       {<=}{{\textcolor{blue}{<=}}}2
       {>=}{{\textcolor{blue}{>=}}}2,
  }
    
    \lstset{style=mystyle}
    \usepackage{fancyhdr}
    
    % Define header and footer styles
    \fancypagestyle{plain}{%
      \fancyhf{}% Clear header/footer
      \fancyhead[L]{Homework 2}% Header left
      \fancyhead[C]{2023/2024}% Header left
      \fancyhead[R]{Aprendizagem}% Header right
      \fancyfoot[C]{\thepage}% Footer center
      \renewcommand{\headrulewidth}{0.4pt}% Header rule
      \renewcommand{\footrulewidth}{0pt}% Footer rule
    }
    
    % Apply the style to all pages except the first one
    \pagestyle{plain}
    \thispagestyle{empty} % Remove header/footer from first page
    
\begin{document}
    
\renewcommand{\arraystretch}{1.7}
\setlength{\columnseprule}{0.4pt}
\tdplotsetmaincoords{70}{110} % Set the viewing angle
\newcolumntype{M}[1]{>{\centering\arraybackslash\vspace{#1}}m{0.5\linewidth}<{\vspace{#1}}}
\newcolumntype{C}[2]{>{\centering\arraybackslash\vspace{#1}\rule{0pt}{#1}\hspace{0pt}}m{#2}}
\ifx\undefined\w
\newcolumntype{w}[1]{>{\centering\arraybackslash}m{#1}}
\fi
\renewcommand*\familydefault{\sfdefault} %% Only if the base font of the document is to be sans serif
\maketitle
\vspace{-5mm}
\hrulefill




\section*{Pen and Paper Exercises}

\section*{Dataset}

The following dataset will be used for this homework:

\begin{table}[h!]
\centering
\begin{tabular}{|cc|ccccc|c|}
\hline
\multicolumn{2}{|c|}{\multirow{2}{*}{$D$}}                           & \multicolumn{5}{c|}{Input}                                                                                                & \multicolumn{1}{l|}{Output} \\ \cline{3-8} 
\multicolumn{2}{|c|}{}                                               & \multicolumn{1}{c|}{$y_1$} & \multicolumn{1}{c|}{$y_2$} & \multicolumn{1}{c|}{$y_3$} & \multicolumn{1}{c|}{$y_4$} & $y_5$ & $y_6$                       \\ \hline
\multicolumn{1}{|c|}{\multirow{7}{*}{Training Observations}} & $x_1$ & \multicolumn{1}{c|}{0.24}  & \multicolumn{1}{c|}{0.36}  & \multicolumn{1}{c|}{1}     & \multicolumn{1}{c|}{1}     & 0     & A                           \\ \cline{2-8} 
\multicolumn{1}{|c|}{}                                       & $x_2$ & \multicolumn{1}{c|}{0.16}  & \multicolumn{1}{c|}{0.48}  & \multicolumn{1}{c|}{1}     & \multicolumn{1}{c|}{0}     & 1     & A                           \\ \cline{2-8} 
\multicolumn{1}{|c|}{}                                       & $x_3$ & \multicolumn{1}{c|}{0.32}  & \multicolumn{1}{c|}{0.72}  & \multicolumn{1}{c|}{0}     & \multicolumn{1}{c|}{1}     & 2     & A                           \\ \cline{2-8} 
\multicolumn{1}{|c|}{}                                       & $x_4$ & \multicolumn{1}{c|}{0.54}  & \multicolumn{1}{c|}{0.11}  & \multicolumn{1}{c|}{0}     & \multicolumn{1}{c|}{0}     & 1     & B                           \\ \cline{2-8} 
\multicolumn{1}{|c|}{}                                       & $x_5$ & \multicolumn{1}{c|}{0.66}  & \multicolumn{1}{c|}{0.39}  & \multicolumn{1}{c|}{0}     & \multicolumn{1}{c|}{0}     & 0     & B                           \\ \cline{2-8} 
\multicolumn{1}{|c|}{}                                       & $x_6$ & \multicolumn{1}{c|}{0.76}  & \multicolumn{1}{c|}{0.28}  & \multicolumn{1}{c|}{1}     & \multicolumn{1}{c|}{0}     & 2     & B                           \\ \cline{2-8} 
\multicolumn{1}{|c|}{}                                       & $x_7$ & \multicolumn{1}{c|}{0.41}  & \multicolumn{1}{c|}{0.53}  & \multicolumn{1}{c|}{0}     & \multicolumn{1}{c|}{1}     & 1     & B                           \\ \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{Testing Observations}}  & $x_8$ & \multicolumn{1}{c|}{0.38}  & \multicolumn{1}{c|}{0.52}  & \multicolumn{1}{c|}{0}     & \multicolumn{1}{c|}{1}     & 0     & A                           \\ \cline{2-8} 
\multicolumn{1}{|c|}{}                                       & $x_9$ & \multicolumn{1}{c|}{0.42}  & \multicolumn{1}{c|}{0.59}  & \multicolumn{1}{c|}{0}     & \multicolumn{1}{c|}{1}     & 1     & B                           \\ \hline
\end{tabular}
\label{tab:dataset1}
\caption{Dataset}
\end{table}

\newpage

\section*{1\textsuperscript{st} Question}

\subsection*{a)}

In order to build the Bayesian classifier for this dataset, we need to compute the class conditional distributions of $\{y_1,y_2\}$, $\{y_3,y_4\}$ and $y_5$, which are the groups of independent input variables of our dataset as well as the priors.

\paragraph{Priors}

First of all, we will compute the priors $P(y_6=A)$ and $P(y_6=B)$:

\begin{align*}
  P(y_6=A) &= \frac{3}{7} \\
  P(y_6=B) &= \frac{4}{7} 
\end{align*}


\paragraph{Distribution of $y_1$ and $y_2$}
\paragraph{}

We are told that $y_1 \times y_2 \in \mathbb{R}$ follows a normal 2D distribution.
A multivariate normal distribution of $m$ variables $\vec{x} = \{x_1, x_2, ..., x_m\}$ is defined by its mean vector $\vec{\mu}$ and its covariance matrix $\Sigma$:

\[
  P(\vec{x}| \vec{\mu}, \Sigma) = \frac{1}{\sqrt{(2\pi)^m |\Sigma|}} \exp \left( -\frac{1}{2} (\vec{x} - \vec{\mu})^T \cdot \Sigma^{-1} \cdot (\vec{x} - \vec{\mu}) \right)
  \]

In our case, we have $m = 2$, $\vec{x} = \{y_1, y_2\}$ and we need to compute two class conditional distributions $p(\vec{x}|y_6=A)$ and $p(\vec{x}|y_6=B)$.

\paragraph{Distribution of $\{y_1,y_2\}$ given $y_6=A$}
\paragraph{}

Considering the training data in table \ref{tab:dataset1} with class $y_6=A$, we can compute the mean vector $\vec{\mu}$ and the covariance matrix $\Sigma$ as follows:

\[
  \vec{\mu} =  \left[\begin{matrix} \mu_{y_1} \\ \mu_{y_2} \end{matrix} \right]= \frac{1}{3} \cdot 
  \left[\begin{matrix}
    0.24 + 0.16 + 0.32 \\
    0.36 + 0.48 + 0.72
  \end{matrix}\right] = \left[\begin{matrix}
    0.24 \\
    0.52
  \end{matrix}\right] \\
\]

\[
  \Sigma = \left[ \begin{matrix}
    \sigma_{y_1}^2 & \sigma_{y_1,y_2} \\
    \sigma_{y_1,y_2} & \sigma_{y_2}^2
  \end{matrix} \right] = \frac{1}{3} \cdot \begin{bmatrix}
    \sum_{i=1}^{3} (y_{1i} - \mu_{y_1})^2 & \sum_{i=1}^{3} (y_{1i} - \mu_{y_1})(y_{2i} - \mu_{y_2}) \\
    \sum_{i=1}^{3} (y_{1i} - \mu_{y_1})(y_{2i} - \mu_{y_2}) & \sum_{i=1}^{3} (y_{2i} - \mu_{y_2})^2
  \end{bmatrix}  = \begin{bmatrix}
    0.0043 & 0.0064 \\
    0.0064 & 0.0224
  \end{bmatrix}
\]

Now we need to compute both $|\Sigma|$ and $\Sigma^{-1}$:

\[
  |\Sigma| = \det \Sigma = 0.0043 \cdot 0.0224 - 0.0064^2 = 5.4613 \cdot 10^{-5}
\]

\[
  \Sigma^{-1} = \begin{bmatrix}
    410.156 & -117.188 \\
    -117.188 & 78.125
  \end{bmatrix}
\]

Therefore, we have the normal distribution of $\{y_1,y_2\}$ given $y_6=A$:

\[
    P((y_1,y_2)|y_6=A) = \frac{1}{\sqrt{(2\pi)^2 |\Sigma|}} \exp \left( -\frac{1}{2} \left(\begin{bmatrix} y_1 \\ y_2 \end{bmatrix} - \begin{bmatrix} 0.24 \\ 0.52 \end{bmatrix}\right)^T \cdot \begin{bmatrix}
    410.156 & -117.188 \\
    -117.188 & 78.125
  \end{bmatrix} \cdot \left(\begin{bmatrix} y_1 \\ y_2 \end{bmatrix} - \begin{bmatrix} 0.24 \\ 0.52 \end{bmatrix}\right) \right)
\]
\[
  = \frac{1}{\sqrt{(2\pi)^2 \cdot 5.4613 \cdot 10^{-5}}} \exp \left( -\frac{1}{2} \left[ \begin{matrix} y_1 - 0.24 \\ y_2 - 0.52 \end{matrix} \right]^T \cdot \begin{bmatrix}
    410.156 & -117.188 \\
    -117.188 & 78.125  
  \end{bmatrix} \cdot \left[ \begin{matrix} y_1 - 0.24 \\ y_2 - 0.52 \end{matrix} \right] \right)
\]




\paragraph{Distribution of $\{y_1,y_2\}$ given $y_6=B$}
\paragraph{}

Considering the training data in table \ref{tab:dataset1} with class $y_6=B$, we can compute the mean vector $\vec{\mu}$ and the covariance matrix $\Sigma$ as follows:

\[
  \vec{\mu} =  \left[\begin{matrix} \mu_{y_1} \\ \mu_{y_2} \end{matrix} \right]= \frac{1}{4} \cdot 
  \left[\begin{matrix}
    0.54 + 0.66 + 0.76 + 0.41 \\
    0.11 + 0.39 + 0.28 + 0.53
  \end{matrix}\right] = \left[\begin{matrix}
    0.5925 \\
    0.3274
  \end{matrix}\right] \\
\]

\[
  \Sigma = \left[ \begin{matrix}
    \sigma_{y_1}^2 & \sigma_{y_1,y_2} \\
    \sigma_{y_1,y_2} & \sigma_{y_2}^2
  \end{matrix} \right] = \frac{1}{4} \cdot \begin{bmatrix}
    \sum_{i=1}^{4} (y_{1i} - \mu_{y_1})^2 & \sum_{i=1}^{4} (y_{1i} - \mu_{y_1})(y_{2i} - \mu_{y_2}) \\
    \sum_{i=1}^{4} (y_{1i} - \mu_{y_1})(y_{2i} - \mu_{y_2}) & \sum_{i=1}^{4} (y_{2i} - \mu_{y_2})^2
  \end{bmatrix}  = \begin{bmatrix}
    0.0171 & -0.0073 \\
    -0.0073 & 0.0236
  \end{bmatrix}
\]

Now we need to compute both $|\Sigma|$ and $\Sigma^{-1}$:

\[
  |\Sigma| = \det \Sigma = 0.0075 \cdot 0.0075 - (-0.0025)^2 = 3.519 \cdot 10^{-4}
\]

\[
  \Sigma^{-1} = \begin{bmatrix}
    67.1101 & 20.7954 \\
    20.7954 & 48.7831
  \end{bmatrix}
\]

Therefore, we have the normal distribution of $\{y_1,y_2\}$ given $y_6=B$:

\[
    P((y_1,y_2)|y_6=B) = \frac{1}{\sqrt{(2\pi)^2 |\Sigma|}} \exp \left( -\frac{1}{2} \left(\begin{bmatrix} y_1 \\ y_2 \end{bmatrix} - \begin{bmatrix} 0.5925 \\ 0.3274 \end{bmatrix}\right)^T \cdot \begin{bmatrix}
    67.1101 & 20.7954 \\
    20.7954 & 48.7831
  \end{bmatrix} \cdot \left(\begin{bmatrix} y_1 \\ y_2 \end{bmatrix} - \begin{bmatrix} 0.5925 \\ 0.3274 \end{bmatrix}\right) \right)
\]

\[
  = \frac{1}{\sqrt{(2\pi)^2 \cdot 3.519 \cdot 10^{-4}}} \exp \left( -\frac{1}{2} \left[ \begin{matrix} y_1 - 0.5925 \\ y_2 - 0.3274 \end{matrix} \right]^T \cdot \begin{bmatrix}
    67.1101 & 20.7954 \\
    20.7954 & 48.7831  
  \end{bmatrix} \cdot \left[ \begin{matrix} y_1 - 0.5925 \\ y_2 - 0.3274 \end{matrix} \right] \right)
\]





\paragraph{Distribution of $y_3$ and $y_4$}
\paragraph{}

The class conditional distributions of $y_3$ and $y_4$ come directly from the information in table \ref{tab:dataset1} and they are given by:

\begin{table}[H]
\centering
\begin{tabular}{|cc|cc|}
\hline
\multicolumn{2}{|c|}{\multirow{2}{*}{$P(y_3 \cap y_4|y_6=A)$}} & \multicolumn{2}{c|}{$y_3$}                     \\ \cline{3-4} 
\multicolumn{2}{|c|}{}                                   & \multicolumn{1}{c|}{0}           & 1           \\ \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{$y_4$}}     & 0     & \multicolumn{1}{c|}{$P(y_3=0 \cap y_4=0|y_6=A) =0$} & $P(y_3=1 \cap y_4=0|y_6=A) = \frac{1}{3}$ \\ \cline{2-4} 
\multicolumn{1}{|c|}{}                           & 1     & \multicolumn{1}{c|}{$P(y_3=0 \cap y_4=1|y_6=A) =\frac{1}{3}$} & $P(y_3=1 \cap y_4=1|y_6=A) =\frac{1}{3}$ \\ \hline
\end{tabular}
\caption{Distribution of $y_3$ and $y_4$ given $y_6=A$}
\end{table}

\begin{table}[H]
\centering
\begin{tabular}{|cc|cc|}
\hline
\multicolumn{2}{|c|}{\multirow{2}{*}{$P(y_3 \cap y_4|y_6=B)$}} & \multicolumn{2}{c|}{$y_3$}                     \\ \cline{3-4}
\multicolumn{2}{|c|}{}                                   & \multicolumn{1}{c|}{0}           & 1           \\ \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{$y_4$}}     & 0     & \multicolumn{1}{c|}{$P(y_3=0 \cap y_4=0|y_6=B) = \frac{1}{2}$} & $P(y_3=1 \cap y_4=0|y_6=B) =\frac{1}{4}$ \\ \cline{2-4} 
\multicolumn{1}{|c|}{}                           & 1     & \multicolumn{1}{c|}{$P(y_3=0 \cap y_4=1|y_6=B) =\frac{1}{4}$} & $P(y_3=1 \cap y_4=1|y_6=B) =0$ \\ \hline
\end{tabular}
\caption{Distribution of $y_3$ and $y_4$ given $y_6=B$}
\end{table}



\paragraph{Distribution of $y_5$}
\paragraph{}

The class conditional distribution of $y_5$ is given by:

\begin{table}[h!]
\centering
\begin{tabular}{|cc|ccc|}
\hline
\multicolumn{2}{|c|}{\multirow{2}{*}{$P(y_5|y_6)$}} & \multicolumn{3}{c|}{$y_5$}                                                                                                        \\ \cline{3-5} 
\multicolumn{2}{|c|}{}                              & \multicolumn{1}{c|}{0}                           & \multicolumn{1}{c|}{1}                           & 2                           \\ \hline
\multicolumn{1}{|c|}{\multirow{2}{*}{$y_6$}}   & A  & \multicolumn{1}{c|}{$P(y_5=0|y_6=A) =\frac{1}{3}$} & \multicolumn{1}{c|}{$P(y_5=1|y_6=A) =\frac{1}{3}$} & $P(y_5=2|y_6=A) =\frac{1}{3}$ \\ \cline{2-5} 
\multicolumn{1}{|c|}{}                         & B  & \multicolumn{1}{c|}{$P(y_5=0|y_6=B) =\frac{1}{4}$} & \multicolumn{1}{c|}{$P(y_5=1|y_6=B) =\frac{1}{2}$} & $P(y_5=2|y_6=B) =\frac{1}{4}$ \\ \hline
\end{tabular}
\caption{Distribution of $y_5$ given $y_6$}
\end{table}

\subsection*{b)}

In order to classify the testing observations, we will need to compute the posterior probabilities. 
Under a MAP assumption, the predicted class for each testing observation is the one that maximizes the posterior probability.
Since we are only interested in the maximum value over all classes, we can ignore the denominator of the posterior probability formula.
We have two testing observations, $x_8$ and $x_9$, and we will compute the posterior probabilities for each of them:

\paragraph{Posterior probabilities for $x_8$}
\paragraph{}

This training observation has the following values for the input variables: $y_1=0.38$, $y_2=0.52$, $y_3=0$, $y_4=1$ and $y_5=0$.

\[ P(y_6=A|x_8) = \frac{P(x_8|y_6=A) \cdot P(y_6=A)}{P(x_8)} \propto P(x_8|y_6=A) \cdot P(y_6=A) = \]
\[ = P(y_1=0.38,y_2=0.52|y_6=A) \cdot P(y_3=0,y_4=1|y_6=A) \cdot P(y_5=0|y_6=A) \cdot P(y_6=A) = \]
\[ = \frac{1}{\sqrt{(2\pi)^2 \cdot 5.4613 \cdot 10^{-5}}} \exp \left( -\frac{1}{2} \left[ \begin{matrix} 0.38 - 0.24 \\ 0.52 - 0.52 \end{matrix} \right]^T \cdot \begin{bmatrix} 410.156 & -117.188 \\ -117.188 & 78.125 \end{bmatrix} \cdot \left[ \begin{matrix} 0.38 - 0.24 \\ 0.52 - 0.52 \end{matrix} \right] \right) \cdot \frac{1}{3} \cdot \frac{1}{3} \cdot \frac{3}{7} = \]
\[ = 0.3868 \cdot \frac{1}{3} \cdot \frac{1}{3} \cdot \frac{3}{7} = 0.01842 \]

\paragraph{}

\[ P(y_6=B|x_8) = \frac{P(x_8|y_6=B) \cdot P(y_6=B)}{P(x_8)} \propto P(x_8|y_6=B) \cdot P(y_6=B) = \]
\[ = P(y_1=0.38,y_2=0.52|y_6=B) \cdot P(y_3=0,y_4=1|y_6=B) \cdot P(y_5=0|y_6=B) \cdot P(y_6=B) = \]
\[ = \frac{1}{\sqrt{(2\pi)^2 \cdot 3.519 \cdot 10^{-4}}} \exp \left( -\frac{1}{2} \left[ \begin{matrix} 0.38 - 0.5925 \\ 0.52 - 0.3274 \end{matrix} \right]^T \cdot \begin{bmatrix} 67.1101 & 20.7954 \\ 20.7954 & 48.7831 \end{bmatrix} \cdot \left[ \begin{matrix} 0.38 - 0.5925 \\ 0.52 - 0.3274 \end{matrix} \right] \right) \cdot \frac{1}{4} \cdot \frac{1}{4} \cdot \frac{4}{7} = \]
\[ = 1.7677 \cdot \frac{1}{4} \cdot \frac{1}{4} \cdot \frac{4}{7} = 0.06313 \]

\paragraph{Posterior probabilities for $x_9$}
\paragraph{}

This training observation has the following values for the input variables: $y_1=0.42$, $y_2=0.59$, $y_3=0$, $y_4=1$ and $y_5=1$.

\[ P(y_6=A|x_9) = \frac{P(x_9|y_6=A) \cdot P(y_6=A)}{P(x_9)} \propto P(x_9|y_6=A) \cdot P(y_6=A) = \]
\[ = P(y_1=0.42,y_2=0.59|y_6=A) \cdot P(y_3=0,y_4=1|y_6=A) \cdot P(y_5=1|y_6=A) \cdot P(y_6=A) = \]
\[ = \frac{1}{\sqrt{(2\pi)^2 \cdot 5.4613 \cdot 10^{-5}}} \exp \left( -\frac{1}{2} \left[ \begin{matrix} 0.42 - 0.24 \\ 0.59 - 0.52 \end{matrix} \right]^T \cdot \begin{bmatrix} 410.156 & -117.188 \\ -117.188 & 78.125 \end{bmatrix} \cdot \left[ \begin{matrix} 0.42 - 0.24 \\ 0.59 - 0.52 \end{matrix} \right] \right) \cdot \frac{1}{3} \cdot \frac{1}{3} \cdot \frac{3}{7} = \]
\[ = 0.1013 \cdot \frac{1}{3} \cdot \frac{1}{3} \cdot \frac{3}{7} = 0.00482 \]

\paragraph{}

\[ P(y_6=B|x_9) = \frac{P(x_9|y_6=B) \cdot P(y_6=B)}{P(x_9)} \propto P(x_9|y_6=B) \cdot P(y_6=B) = \]
\[ = P(y_1=0.42,y_2=0.59|y_6=B) \cdot P(y_3=0,y_4=1|y_6=B) \cdot P(y_5=1|y_6=B) \cdot P(y_6=B) = \]
\[ = \frac{1}{\sqrt{(2\pi)^2 \cdot 3.519 \cdot 10^{-4}}} \exp \left( -\frac{1}{2} \left[ \begin{matrix} 0.42 - 0.5925 \\ 0.59 - 0.3274 \end{matrix} \right]^T \cdot \begin{bmatrix} 67.1101 & 20.7954 \\ 20.7954 & 48.7831 \end{bmatrix} \cdot \left[ \begin{matrix} 0.42 - 0.5925 \\ 0.59 - 0.3274 \end{matrix} \right] \right) \cdot \frac{1}{4} \cdot \frac{1}{2} \cdot \frac{4}{7} = \]
\[ = 1.4927 \cdot \frac{1}{4} \cdot \frac{1}{2} \cdot \frac{4}{7} = 0.05331 \]

\paragraph{Predicted classes}
\paragraph{}

Organizing the posterior probabilities in a table, we have:

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
Observation & $P(y_6=A|x_i)$ & $P(y_6=B|x_i)$ \\ \hline
$x_8$       & 0.01842         & \textbf{0.06313}        \\ \hline
$x_9$       & 0.00482         & \textbf{0.05331}        \\ \hline
\end{tabular}
\label{tab:posterior_probabilities}
\caption{Posterior probabilities for the testing observations}
\end{table}

Therefore, the predicted class for both $x_8$ and $x_9$ is $y_6=B$.

\subsection*{c)}

Let's consider the following classifier with a unknown treshold $\theta$ whose value we aim to find:

\[
  f(x|\theta) = \begin{cases}
    A & \text{if } P(y_6=A|x) > \theta \\
    B & \text{if } P(y_6=A|x) \leq \theta
  \end{cases}
\]

\paragraph{Finding $P(y_6=A|x)$}
\paragraph{}

We are now working under a ML assumption, so, in order to classify each test observation we will only need the conditional distribution $P(x|y_6=A)$ because every class has the same prior probability.
We used the values in the previous section and divided them by the corresponding a priori probability:

\[ P(y_6 = A|x_8) = 0.04298 \]
\[ P(y_6 = A|x_9) = 0.01126 \]

\paragraph{Accuracy}

The accuracy of a classifier is given by:

\[ \text{Accuracy} = \frac{TP + TN}{TP + TN + FP + FN} \]

where $TP$ is the number of true positives, $TN$ is the number of true negatives, $FP$ is the number of false positives and $FN$ is the number of false negatives.
In our case, accuracy can only have three possible values: 0, 0.5 or 1: 0 when both $x_8$ and $x_9$ are misclassified, 0.5 when only one of them is misclassified and 1 when both are correctly classified.
In order to maximize the accuracy, we want it to be 1. We know from table \ref{tab:dataset1} that $x_8$ belongs to class $A$ and $x_9$ belongs to class $B$. Therefore, in order to maximize the accuracy, our classifier needs to classify $x_8$ as $A$ and $x_9$ as $B$.
$f(x_8|\theta) = A$ therefore $\theta < P(y_6=A|x_8) = 0.04298$ and $f(x_9|\theta) = B$ therefore $\theta > P(y_6=A|x_9) = 0.01126$. 

\[ \theta \in ]0.01126; 0.04298[ \]

Any value of $\theta$ in this interval will maximize the accuracy of our classifier.

\newpage

\section*{2\textsuperscript{nd} Question}

\subsection*{a)}

In order to obtain $y_2$ under an equal-range discretization, we followed the rule:

\[ y_{2_{\text{normalized}}} = \begin{cases}
  0 & \text{if } y_2 \in [0, 0.5) \\
  1 & \text{if } y_2 \in [0.5, 1]
\end{cases} \]

The normalized values are:

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|}
  \hline
  Dataset & $x_1$ & $x_2$ & $x_3$ & $x_4$ & $x_5$ & $x_6$ & $x_7$ & $x_8$ & $x_9$ \\ \hline
  $y_2$ & 0 & 0 & 1 & 0 & 0 & 0 & 1 & 1 & 1 \\ \hline
\end{tabular}
\label{tab:dataset2}
\caption{Normalized $y_2$ values}
\end{table}

$y_1$ will now be considered an output variable and $y_2$ to $y_6$ will be considered input variables.
Considering the normalized values of $y_2$, we can rewrite the dataset as follows:

\begin{table}[H]
\centering
\begin{tabular}{|cc|c|ccccc|}
\hline
\multicolumn{2}{|c|}{D}                              & Output & \multicolumn{5}{c|}{Input}                                                                                                         \\ \hline
\multicolumn{1}{|c|}{Fold}                   &       & $y_1$  & \multicolumn{1}{c|}{$y_{2_{norm}}$} & \multicolumn{1}{c|}{$y_3$} & \multicolumn{1}{c|}{$y_4$} & \multicolumn{1}{c|}{$y_5$} & $y_6$ \\ \hline
\multicolumn{1}{|c|}{\multirow{3}{*}{$F_1$}} & $x_1$ & 0.24   & \multicolumn{1}{c|}{0}              & \multicolumn{1}{c|}{1}     & \multicolumn{1}{c|}{1}     & \multicolumn{1}{c|}{0}     & A     \\ \cline{2-8} 
\multicolumn{1}{|c|}{}                       & $x_2$ & 0.16   & \multicolumn{1}{c|}{0}              & \multicolumn{1}{c|}{1}     & \multicolumn{1}{c|}{0}     & \multicolumn{1}{c|}{1}     & A     \\ \cline{2-8} 
\multicolumn{1}{|c|}{}                       & $x_3$ & 0.32   & \multicolumn{1}{c|}{1}              & \multicolumn{1}{c|}{0}     & \multicolumn{1}{c|}{1}     & \multicolumn{1}{c|}{2}     & A     \\ \hline
\multicolumn{1}{|c|}{\multirow{3}{*}{$F_2$}} & $x_4$ & 0.54   & \multicolumn{1}{c|}{0}              & \multicolumn{1}{c|}{0}     & \multicolumn{1}{c|}{0}     & \multicolumn{1}{c|}{1}     & B     \\ \cline{2-8} 
\multicolumn{1}{|c|}{}                       & $x_5$ & 0.66   & \multicolumn{1}{c|}{0}              & \multicolumn{1}{c|}{0}     & \multicolumn{1}{c|}{0}     & \multicolumn{1}{c|}{0}     & B     \\ \cline{2-8} 
\multicolumn{1}{|c|}{}                       & $x_6$ & 0.76   & \multicolumn{1}{c|}{0}              & \multicolumn{1}{c|}{1}     & \multicolumn{1}{c|}{0}     & \multicolumn{1}{c|}{2}     & B     \\ \hline
\multicolumn{1}{|c|}{\multirow{3}{*}{$F_3$}} & $x_7$ & 0.41   & \multicolumn{1}{c|}{1}              & \multicolumn{1}{c|}{0}     & \multicolumn{1}{c|}{1}     & \multicolumn{1}{c|}{1}     & B     \\ \cline{2-8} 
\multicolumn{1}{|c|}{}                       & $x_8$ & 0.38   & \multicolumn{1}{c|}{1}              & \multicolumn{1}{c|}{0}     & \multicolumn{1}{c|}{1}     & \multicolumn{1}{c|}{0}     & A     \\ \cline{2-8} 
\multicolumn{1}{|c|}{}                       & $x_9$ & 0.42   & \multicolumn{1}{c|}{1}              & \multicolumn{1}{c|}{0}     & \multicolumn{1}{c|}{1}     & \multicolumn{1}{c|}{1}     & B     \\ \hline
\end{tabular}
\caption{Dataset $D$ divided into three folds}
\label{tab:dataset2_folds}
\end{table}

Adittionally, we have divided the dataset into three folds, $F_1$, $F_2$ and $F_3$.

\subsection*{b)}

In this exercise we aim to compute a kNN (k  nearest neighbors - Lazy Learning) classifier considering the following parameters:

\begin{itemize}
  \item $k = 3$
  \item \textbf{Hamming Distance} as the distance to be used to compute the nearest neighbors of a given observation.
  \[ d_H(x_i,x_j) = \sum_{l=1}^{m} \delta(y_{il},y_{jl}) \]
  where $m$ is the number of input variables and $y_{il}$ is the value of the $l$\textsuperscript{th} input variable of the $i$\textsuperscript{th} observation.
\end{itemize}

We have divided our dataset in folds in order to perform a cross validation. We will only be interested in the first iteration of the cross validation, where $F_3$ is the testing fold and $F_1$ and $F_2$ are the training folds. 
We will now compute the kNN classifier for each observation in $F_3$ and afterwards we will compute the MAE (Mean Absolute Error) for the testing fold.

\subsubsection*{Computing the Hamming Distances}

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|c|c|c|c|}
  \hline
  Testing Observation ($x_i$) & $d_H(x_i,x_1)$ & $d_H(x_i,x_2)$ & $d_H(x_i,x_3)$ & $d_H(x_i,x_4)$ & $d_H(x_i,x_5)$ & $d_H(x_i,x_6)$ \\ \hline
  $x_7$                       & 4              & 4              & 2 \cellcolor{yellow!25}             & 2 \cellcolor{yellow!25}             & 3 \cellcolor{yellow!25}             & 4              \\ \hline
  $x_8$                       & 2 \cellcolor{yellow!25}             & 4              & 1 \cellcolor{yellow!25}             & 4              & 3 \cellcolor{yellow!25}             & 5              \\ \hline
  $x_9$                       & 4              & 4              & 2 \cellcolor{yellow!25}             & 2  \cellcolor{yellow!25}            & 3 \cellcolor{yellow!25}             & 4              \\ \hline
\end{tabular}
\caption{Hamming distances between the testing observation $x_i$ and the training observations $x_j$}
\label{tab:hamming_distances}
\end{table}

We are considering $k=3$, so we will only need the three nearest neighbors of each testing observation. In the table above we have filled with yellow the three nearest neighbors of each testing observation.

\subsubsection*{Predicted value of $y_1$ for each testing observation}

The output value we are working with is numerical, so the predicted value of $y_1$ for each testing observation will be:

\[ \hat{y_1}_j = \frac{\sum_{i=1}^{k} \frac{1}{d_H(x_i,x_j)} \cdot y_{1j}}{\sum_{i=1}^{k} \frac{1}{d_H(x_i,x_j)}} \]

where $k$ is the number of nearest neighbors, $d_H(x_i,x_j)$ is the Hamming distance between the testing observation $x_i$ and the $i$\textsuperscript{th} nearest neighbor and $y_{1j}$ is the value of the output variable of the $j$\textsuperscript{th} nearest neighbor.
$\frac{1}{d_H(x_i,x_j)}$ is the weight  of the $i$\textsuperscript{th} nearest neighbor.

\[ \hat{y_1}_7 = \frac{1}{\frac{1}{2} + \frac{1}{2} + \frac{1}{3}} \cdot \left( \frac{1}{2} \cdot 0.32 + \frac{1}{2} \cdot 0.54 + \frac{1}{3} \cdot 0.66 \right) = 0.4875 \]
\[ \hat{y_1}_8 = \frac{1}{\frac{1}{2} + \frac{1}{1} + \frac{1}{3}} \cdot \left( \frac{1}{2} \cdot 0.24 + 1 \cdot 0.32 + \frac{1}{3} \cdot 0.66 \right) = 0.36 \]
\[ \hat{y_1}_9 = \frac{1}{\frac{1}{2} + \frac{1}{2} + \frac{1}{3}} \cdot \left( \frac{1}{2} \cdot 0.32 + \frac{1}{2} \cdot 0.54 + \frac{1}{3} \cdot 0.66 \right) = 0.4875 \]

We have the following predicted values for $y_1$:

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
  \hline
  Testing Observation ($x_i$) & $\hat{y_1}_i$ & $y_{1i}$ \\ \hline
  $x_7$                       & 0.4875        & 0.41     \\ \hline
  $x_8$                       & 0.36          & 0.38     \\ \hline
  $x_9$                       & 0.4875        & 0.42     \\ \hline
\end{tabular}
\caption{Predicted values of $y_1$ for each testing observation}
\label{tab:predicted_y1}
\end{table}

\subsubsection*{MAE}

The MAE is given by:

\[ \text{MAE} = \frac{1}{n} \sum_{i=1}^{n} |y_{1i} - \hat{y_{1i}}| \]

where $n$ is the number of testing observations, $y_{1i}$ is the value of the output variable of the $i$\textsuperscript{th} testing observation and $\hat{y_{1i}}$ is the predicted value of the output variable of the $i$\textsuperscript{th} testing observation.

\[ \text{MAE} = \frac{1}{3} \cdot (|0.41 - 0.4875| + |0.38 - 0.36| + |0.42 - 0.4875|) = 0.055 \]

\newpage

\section*{Programming and Critical Analysis}

\subsection*{Imports}
\begin{lstlisting}[language=Python]
  # Sklearn Imports
  import sklearn as sk
  from sklearn.metrics import confusion_matrix
  from sklearn.model_selection import cross_val_score, StratifiedKFold
  from sklearn.neighbors import KNeighborsClassifier
  from sklearn.naive_bayes import GaussianNB
  # Other Imports
  import numpy as np
  import pandas as pd
  import matplotlib.pyplot as plt
  import seaborn as sns
  from pathlib import Path
  from scipy.io.arff import loadarff
  from scipy import stats
\end{lstlisting}

\subsection*{Loading DataSet and Define Directories}

\begin{lstlisting}[language=Python]
  IMAGES_DIR = Path('images')
  IMAGES_DIR.mkdir(parents=True, exist_ok=True)
  DATA_DIR = Path('data')
  DATA_DIR.mkdir(parents=True, exist_ok=True)
  DATA_FILE = 'column_diagnosis.arff'
  DATA_PATH = DATA_DIR / DATA_FILE
  data = loadarff(DATA_PATH)
  df = pd.DataFrame(data[0])
  df['class'] = df['class'].str.decode('utf-8')
  # Show the first 5 rows
  df.head()
  
\end{lstlisting}

\subsection*{1\textsuperscript{st} Question}

\subsubsection*{10-fold cross validation with suffling }

  \begin{lstlisting}[language=Python]
    # Split into features and labels 
    X = df.drop('class', axis=1)
    y = df['class']
  \end{lstlisting}

  \begin{lstlisting}[language=Python]
    # Stratified 10 fold cross validation
    cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=0)
  \end{lstlisting}

\subsubsection*{Box Plots for Fold Accuracy}
  \begin{lstlisting}[language=Python]
    # kNN Classifier with 5 neighbors
    knn = KNeighborsClassifier(n_neighbors=5)

    # Naive Bayes Classifier
    nb = GaussianNB()

    # 10-fold startified cross validation with shuffle
    knn_score = cross_val_score(knn,
                                X,
                                y,
                                cv=cv,
                                scoring='accuracy')

    nb_score = cross_val_score(nb,
                                X,
                                y,
                                cv=cv,
                                scoring='accuracy')
  \end{lstlisting}

  \begin{lstlisting}[language=Python]
    # Create boxplots for the accuracies of both classifiers
    plt.boxplot([knn_score, nb_score], labels=['k-NN', 'Naive Bayes'])
    plt.ylabel('Accuracy')
    plt.title('Accuracy Comparison Between k-NN and Naive Bayes')
    plt.savefig(IMAGES_DIR / 'boxplot.png')
    plt.show()
  \end{lstlisting}

  \begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/boxplot.png}
    \caption{Boxplot for the accuracies of both classifiers}
    \label{fig:boxplot}
  \end{figure}

  \textbf{Comment:}
  As seen by the box plot above, both models have an equal central tendency, shown by the median. Beyond that, the variance of the models are quite similiar, since the overall amplitude of the box plots are similiar. However, Naive Bayes has a variance towards the lower end of the box plot, meaning it has a tendency for lower accuracy.

  \begin{lstlisting}[language=Python]
    # Perform a two-sample t-test to compare the means
  t_stat, p_value = stats.ttest_rel(knn_score, nb_score, alternative='greater')
  print(f"t-statistic: {str(t_stat)}")
  print(f"p-value: {str(p_value)}")

  # Determine if the null hypothesis can be rejected (p < 0.05)
  alpha = 0.05
  if p_value < alpha:
      print("Null hypothesis rejected: \nkNN is statistically superior to Naive Bayes.")
  else:
      print("Null hypothesis not rejected: \nNo significant difference between kNN and Naive Bayes.")
  \end{lstlisting}

  \begin{lstlisting}
    t-statistic: 0.9214426752509264
    p-value: 0.19042809062064092
    Null hypothesis not rejected: 
    No significant difference between kNN and Naive Bayes.
  \end{lstlisting}




\subsection*{2\textsuperscript{nd} Question}

\begin{lstlisting}[language=Python]
  # Create k-NN classifiers with k=1 and k=5
  knn_k1 = KNeighborsClassifier(n_neighbors=1, weights='uniform', metric='euclidean')
  knn_k5 = KNeighborsClassifier(n_neighbors=5, weights='uniform', metric='euclidean')

  # Create a confusion matrix for each classifier
  confusion_matrix_k1 = np.zeros((3, 3))
  confusion_matrix_k5 = np.zeros((3, 3))

  for train_index, test_index in cv.split(X, y):
      # Split the data into training and testing sets
      X_train, X_test = X.iloc[train_index], X.iloc[test_index]
      y_train, y_test = y.iloc[train_index], y.iloc[test_index]

      # Fit both models to the dataset
      knn_k1.fit(X_train, y_train)
      knn_k5.fit(X_train, y_train)

      # Make predictions using both models
      predictions_k1 = knn_k1.predict(X_test)
      predictions_k5 = knn_k5.predict(X_test)

      # Add the confusion matrices to the cumulative confusion matrices
      confusion_matrix_k1 += confusion_matrix(y_test, predictions_k1)
      confusion_matrix_k5 += confusion_matrix(y_test, predictions_k5)

  # Plot the differences between the cumulative confusion matrices
  # Get the class names
  class_names = np.unique(y)

  # Calculate the difference between the cumulative confusion matrices
  confusion_matrix_difference = confusion_matrix_k1 - confusion_matrix_k5

  # Plot the differences between the cumulative confusion matrices
  plt.figure(figsize=(8, 6))
  plt.imshow(confusion_matrix_difference, cmap='coolwarm', interpolation='nearest')
  plt.colorbar(label='Difference')
  plt.title('Confusion Matrix Difference (k=1 - k=5)')
  plt.xticks(ticks=np.arange(len(class_names)), labels=class_names)
  plt.yticks(ticks=np.arange(len(class_names)), labels=class_names)
  plt.xlabel('Predicted Class')
  plt.ylabel('True Class')

  # Loop over the cells and add the values to the plot
  for i in range(len(class_names)):
      for j in range(len(class_names)):
          plt.text(j, i, str(confusion_matrix_difference[i, j]), horizontalalignment='center', verticalalignment='center')
  
  plt.savefig(IMAGES_DIR / 'confusion_matrix_difference.png')
  plt.show()
\end{lstlisting}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{images/confusion_matrix_difference.png}
  \caption{Confusion Matrix Difference (k=1 - k=5)}
  \label{fig:confusion_matrix_difference}
\end{figure}

\textbf{Comment:}
As seen by the difference confusion cummulative matrix above, the classifiers do not have significant differences, since the highest difference between them is 5. As for some entraces of the matrix, one can say both classifiers have the same performance, since the difference between them is 0. For negative values on the diagonal, it means that the classifier knn5 has a better performance than the classifier knn1, and for positive values, the opposite. As one can see, what stands out prominently in the matrix is the prevalence of positive values, outnumbering the negative ones. This observation implies that k-NN5 demonstrates superior performance in distinguishing between classes and encounters fewer instances of confusion compared to k-NN1.





\subsection*{3\textsuperscript{rd} Question}

There are some problems that Naive Bayes may encounter. The tree main problems we can think of are:
\begin{itemize}
  \item Independence Assumption
  \item Continuous Features
  \item Imbalanced Classes
\end{itemize}

\textbf{Independence Assumption}

Naive Bayes assumes that features are conditionally independent given the class label. In other words, it assumes that there are no correlations between the features. However, in real-world datasets like medical diagnoses, features may be correlated. For example, certain symptoms or medical test results might be related. If there are strong correlations between features, Naive Bayes may not capture these relationships accurately, leading to suboptimal performance.


\textbf{Continuous Feature} 

If the dataset contains continuous or numerical features, Naive Bayes relies on assuming that the data follows a specific probability distribution (e.g., Gaussian for Gaussian Naive Bayes). If the data distribution significantly deviates from this assumption, the model's performance can degrade. Medical datasets often contain a mix of continuous and categorical features, and handling continuous data can be a challenge for Naive Bayes.


\textbf{Imbalanced Classes} 

If the dataset has imbalanced class distributions, where one class significantly outnumbers the others, Naive Bayes can be biased towards the majority class. This is because the class prior probabilities heavily influence the classification decision. In medical datasets, it's common to have imbalanced class distributions, such as a rare disease. Naive Bayes may struggle to correctly classify the minority class due to the bias.

To address if those problems exist in our dataset, we can do the following:
\begin{itemize}
  \item Check if there are correlations between features
  \item Check if the data follows the assumed probability distribution
  \item Check if the class distributions are balanced
\end{itemize}

We do that in the following code:

\subsubsection*{Independence Assumption}
  \begin{lstlisting}[language=Python]
    # Independence Assumption
    # Calculate pairwise feature correlations
    correlation_matrix = X.corr()

    # Visualize feature correlations using a heatmap
    plt.figure(figsize=(10, 8))
    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
    plt.title("Feature Correlation Heatmap")
    plt.savefig(IMAGES_DIR / 'correlation_heatmap.png')
    plt.show()
  \end{lstlisting}

  \begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{images/correlation_heatmap.png}
    \caption{Feature Correlation Heatmap}
    \label{fig:correlation_heatmap}
  \end{figure}

  \textbf{Comment:}
  As one can see in the correlation matrix, there are some features that are correlated.
  For example, the features "sacral slope" and "pelvic incidence" are correlated with a correlation coefficient of 0.81,
  which is considered as a strong correlation. This means that Naive Bayes may not be the best choice for this dataset,
  since it assumes that features are independent.

\subsubsection*{Continuous Features}

\begin{lstlisting}[language=Python]
  # Continuous Features
  # Identify continuous features (assuming all non-integer features are continuous)
  continuous_features = [col for col in X.columns if X[col].dtype != 'int64']

  # Plot histograms for continuous features
  num_cols = 3
  num_rows = int(np.ceil(len(continuous_features) / num_cols))
  fig, axs = plt.subplots(num_rows, num_cols, figsize=(20, 4*num_rows))

  for i, feature in enumerate(continuous_features):
      row = i // num_cols
      col = i % num_cols
      sns.histplot(data=X, x=feature, kde=True, color='skyblue', ax=axs[row][col])
      axs[row][col].set_title(f'Distribution of {feature}')

  plt.savefig(IMAGES_DIR / 'continuous_features.png')
  plt.show()
\end{lstlisting}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{images/continuous_features.png}
  \caption{Distribution of continuous features}
  \label{fig:continuous_features}
\end{figure}

\textbf{Comment:}
As one can see in the above plot, almost all features resemble a Gaussian distribution. However, there are some features that do not follow a Gaussian distribution. For example, the Degree Spondylolisthesis is close to the origin, more like a Gamma distribution. This may cause problems for Naive Bayes, since it assumes a Gaussian distribution. To resolve this possible issue, one can try to preprocess the data in order to make it more Gaussian-like.

\subsubsection*{Imbalanced Classes}

\begin{lstlisting}[language=Python]
  # Imbalanced Classes
  # Count class frequencies
  class_counts = y.value_counts()

  # Visualize class distribution
  plt.figure(figsize=(6, 4))
  sns.barplot(x=class_counts.index, y=class_counts.values, palette="Set2")
  plt.title("Class Distribution")
  plt.xlabel("Class")
  plt.ylabel("Count")
  plt.savefig(IMAGES_DIR / 'class_distribution.png')
  plt.show()

  # Output class frequencies
  print("Class Frequencies:")
  print(class_counts)
\end{lstlisting}

\begin{figure}[H]
  \centering
  \includegraphics[width=0.8\textwidth]{images/class_distribution.png}
  \caption{Class Distribution}
  \label{fig:class_distribution}
\end{figure}

\begin{lstlisting}
  Class Frequencies:
  class
  Spondylolisthesis    150
  Normal               100
  Hernia                60
  Name: count, dtype: int64
\end{lstlisting}

\textbf{Comment:}
As one can see in the above plot, the class distributions do not seem to be balanced. The Spondylolisthesis class significantly outnumbers the other two classes, making it more than the double of the Hernia class and 50\% more than the Normal Class. This may cause problems for Naive Bayes, since it can be biased towards the majority class. To resolve this possible issue, one can try to balance the class distributions by either undersampling the majority class or oversampling the minority classes.

\end{document}