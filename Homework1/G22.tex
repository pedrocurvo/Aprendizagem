
% RUN:
% pdflatex -output-directory=/home/salvadortorpes/Aprendizagem/Homework1/trash /home/salvadortorpes/Aprendizagem/Homework1/G22.tex

% ir a settings.json e adicionar:
% // According to the wiki, the string latex-workshop.latex.autoBuild.run has three possible values: never, onSave and onFileChange(default).
% "latex-workshop.latex.autoBuild.run": "never",

\documentclass{article}

\author{Pedro Curvo (ist1102716) \\ Salvador Torpes (ist1102474)}

\usepackage[utf8]{inputenc}
\usepackage[portuguese]{babel}
\usepackage[letterpaper,top=10mm,bottom=15mm,left=10mm,right=10mm,marginparwidth=1.75cm]{geometry}
\usepackage{multicol}
\usepackage{biblatex}
\addbibresource{Bibliografia.bib}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{array}
\usepackage{makecell}
\usepackage{multirow}
\usepackage{amsmath}
\usepackage{makecell}
\usepackage{url}
\usepackage{csquotes}
\usepackage{caption}
\usepackage{enumitem}
\usepackage{textcomp}
\usepackage{pdflscape}
\usepackage{makeidx}
% \usepackage{tocbibind}
\providecommand{\tightlist}{\relax}
\usepackage{tocloft}
\renewcommand{\cftsecindent}{0em}
\renewcommand{\cftsubsecindent}{1em}
\renewcommand{\cftsecfont}{\bfseries}
\renewcommand{\cftsubsecfont}{\itshape}
\setlength{\cftsubsecnumwidth}{0em}

\usepackage[version=4]{mhchem}
\usepackage{hyperref} % Remove "pdftex" option here
\usepackage{float}
\usepackage{fancyhdr}
\usepackage{ragged2e}
\usepackage{xkeyval}
%\usepackage{minted}
%\usemintedstyle{manni}
\usepackage{listings}
\usepackage{amssymb}



\usepackage{xcolor}
\usepackage{tikz}
\usetikzlibrary{positioning}
\usetikzlibrary{positioning, arrows.meta}
\usepackage{adjustbox}
\usepackage{sidecap}
\usepackage{graphicx}

\usepackage{tikz-3dplot}
% \usepackage{pgfplots}
\usetikzlibrary{calc, 3d, arrows}
\usepackage{forest}




\usetikzlibrary{shapes.geometric, arrows}


\lstset{
    language=Python,
    basicstyle=\ttfamily,
    keywordstyle=\color{blue},
    commentstyle=\color{gray},
    stringstyle=\color{orange},
    numbers=left,
    numberstyle=\tiny,
    numbersep=5pt,
    showspaces=false,
    showstringspaces=false,
    breaklines=true,
    frame=tb,
    framexleftmargin=2em,
    xleftmargin=2em,
}


%\usepackage{fontspec}

%\setmonofont{Fira Code}

\fancyhf{}
\cfoot{\thepage}
\fancyhf{} % Clear all header and footer fields
\renewcommand{\headrulewidth}{0pt} % Remove the header rule line
\cfoot{\thepage} % Set the page number in the center of the footer

\pagestyle{fancy} % Apply the fancy page style

\setlength\columnsep{20pt}

\renewcommand{\familydefault}{\sfdefault}

\newenvironment{Figure}
  {\par\medskip\noindent\minipage{\linewidth}}
  {\endminipage\par\medskip}

\makeatletter
\newenvironment{figurehere}
{\def\@captype{figure}}
{}
\makeatother

\hypersetup{
  colorlinks,
  linkcolor=blue,
  anchorcolor=black,
  citecolor=cyan,
  filecolor=cyan,
  menucolor=cyan,
  urlcolor=cyan,
  bookmarksopen=true,
  bookmarksnumbered=true
}

\makeindex


\title{\vspace{-6mm}\includegraphics[width=15mm,scale=3]{Homework1/images/IST_Logo.png}\\ \vspace{5mm}
Aprendizagem - HomeWork 1 \vspace{-5mm}}
\date{1º Semestre - 23/24}

\usepackage{sansmathfonts}
\usepackage[T1]{fontenc}
\usepackage[OT1]{fontenc}

\begin{document}

\renewcommand{\arraystretch}{1.5}
\setlength{\columnseprule}{0.4pt}
\tdplotsetmaincoords{70}{110} % Set the viewing angle
\newcolumntype{M}[1]{>{\centering\arraybackslash\vspace{#1}}m{0.5\linewidth}<{\vspace{#1}}}
\newcolumntype{C}[2]{>{\centering\arraybackslash\vspace{#1}\rule{0pt}{#1}\hspace{0pt}}m{#2}}
\newcolumntype{w}[1]{>{\centering\arraybackslash}m{#1}}

\renewcommand*\familydefault{\sfdefault} %% Only if the base font of the document is to be sans serif



\maketitle

\vspace{-5mm}

\hrulefill

\section{Dataset}

Considering dataset D:

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|c|c|c|}
\hline
    D     & $y_1$ & $y_2$ & $y_3$ & $y_4$ & $y_{out}$ \\ \hline
    $x_1$ & 0.24   & 1     & 1     & 0     & A         \\ \hline
    $x_2$ & 0.06   & 2     & 0     & 0     & B         \\ \hline
    $x_3$ & 0.04   & 0     & 0     & 0     & B         \\ \hline
    $x_4$ & 0.36   & 0     & 2     & 1     & C         \\ \hline
    $x_5$ & 0.32   & 0     & 0     & 2     & C         \\ \hline
    $x_6$ & 0.68   & 2     & 2     & 1     & A         \\ \hline
    $x_7$ & 0.90   & 0     & 1     & 2     & A         \\ \hline
    $x_8$ & 0.76   & 2     & 2     & 0     & A         \\ \hline
    $x_9$ & 0.46   & 1     & 1     & 1     & B         \\ \hline
 $x_{10}$ & 0.62   & 0     & 0     & 1     & B         \\ \hline
 $x_{11}$ & 0.44   & 1     & 2     & 2     & C         \\ \hline
 $x_{12}$ & 0.52   & 0     & 2     & 0     & C         \\ \hline
\end{tabular}
\caption{Dataset D}
\label{tab:datasetD}
\end{table}

\section{Exercício 1.}

De modo a corretamente completar a árvore de decisão, é necessário calcular o Information gain (IG) da variável de output $y_{out}$ condicionada a cada uma das variáveis $y_2$, $y_3$ e $y_4$:

\subsection{Information Gain de $y_{out}$ condicionada a $y_2$}

\[ IG(y_{out}|y_2) = H(y_{out}) - H(y_{out}|y_2) \]

\[ H(y_{out}) = \left(- \sum_{i=1}^{3} p_{out_i} (\log_2 p_{out_i})\right) = - \left( \frac{4}{12} \log_2 \left( \frac{4}{12} \right) + \frac{4}{12} \log_2 \left( \frac{4}{12} \right) + \frac{4}{12} \log_2 \left( \frac{4}{12} \right) \right) = 1.58496 \]

\[ H(y_{out}|y_2) = \sum_{i=0}^{2} p_{y_2 = i} H(y_{out}|y_2 = i) \]

Tabela dividida em 3 sub-tabelas, cada uma com os dados que verificam $y_2 = 0$, $y_2 = 1$ e $y_2 = 2$, respetivamente:

\begin{multicols}{3}
  \setlength{\columnseprule}{0pt}
  
  \begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|}
    \hline
        D     & $y_2$ & $y_{out}$ \\ \hline
        $x_3$ & 0     & B         \\ \hline
        $x_4$ & 0     & C         \\ \hline
        $x_5$ & 0     & C         \\ \hline
        $x_7$ & 0     & A         \\ \hline
        $x_{10}$ & 0     & B         \\ \hline
        $x_{12}$ & 0     & C         \\ \hline
    \end{tabular}
    \caption{Dataset D com $y_2 = 0$}
    \label{tab:datasetDy2=0}
    \end{table}

\vspace*{\fill}
\collumnbreak

\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|}
  \hline
      D     & $y_2$ & $y_{out}$ \\ \hline
      $x_1$ & 1     & A         \\ \hline
      $x_9$ & 1     & B         \\ \hline
      $x_{11}$ & 1     & C         \\ \hline
  \end{tabular}
  \caption{Dataset D com $y_2 = 1$}
  \label{tab:datasetDy2=1}
  \end{table}

\vspace*{\fill}
\collumnbreak

\begin{table}[H]
  \centering
  \begin{tabular}{|c|c|c|}
  \hline
      D     & $y_2$ & $y_{out}$ \\ \hline
      $x_1$ & 1     & A         \\ \hline
      $x_9$ & 1     & B         \\ \hline
      $x_{11}$ & 1     & C         \\ \hline
  \end{tabular}
  \caption{Dataset D com $y_2 = 1$}
  \label{tab:datasetDy2=1}
  \end{table}

\end{multicols}

\[ H(y_{out}|y_2 = 0) = - \left( \frac{1}{6} \log_2 \left( \frac{1}{6} \right) + \frac{2}{6} \log_2 \left( \frac{2}{6} \right) + \frac{1}{2} \log_2 \left( \frac{1}{2} \right) \right) = 1.45915 \]

\[ H(y_{out}|y_2 = 1) = - \left( \frac{1}{3} \log_2 \left( \frac{1}{3} \right) + \frac{1}{3} \log_2 \left( \frac{1}{3} \right) + \frac{1}{3} \log_2 \left( \frac{1}{3} \right) \right) = 1.58496 \]

\[ H(y_{out}|y_2 = 2) = - \left( \frac{1}{3} \log_2 \left( \frac{1}{3} \right) + \frac{2}{3} \log_2 \left( \frac{2}{3} \right) \right) = 0.9183 \]

Assim, podemos calcular a entropia de $y_{out}$ condicionada a $y_2$:

\[ H(y_{out}|y_2) = \frac{6}{12} H(y_{out}|y_2 = 0) + \frac{3}{12} H(y_{out}|y_2 = 1) + \frac{3}{12} H(y_{out}|y_2 = 2) = \]
\[ = \frac{6}{12} \times 1.45915 + \frac{3}{12} \times 1.58496 + \frac{3}{12} \times 0.9183 = 1.35538 \]

Por fim, podemos calcular o Information Gain:

\[ IG(y_{out}|y_2) = H(y_{out}) - H(y_{out}|y_2) = 1.58496 - 1.35538 = 0.22958 \]

\subsection{Information Gain de $y_{out}$ condicionada a $y_3$}

\[ IG(y_{out}|y_3) = H(y_{out}) - H(y_{out}|y_3) \]

\[ H(y_{out}|y_3) = \sum_{i=0}^{2} p_{y_3 = i} H(y_{out}|y_3 = i) \]

Tabela dividida em 3 sub-tabelas, cada uma com os dados que verificam $y_3 = 0$, $y_3 = 1$ e $y_3 = 2$, respetivamente:

\begin{multicols}{3}
\setlength{\columnseprule}{0pt}

\vspace*{\fill}
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
D     & $y_3$ & $y_{out}$ \\ \hline
$x_2$ & 0     & B         \\ \hline
$x_3$ & 0     & B         \\ \hline
$x_5$ & 0     & C         \\ \hline
$x_{10}$ & 0     & B         \\ \hline
\end{tabular}
\caption{Dataset D com $y_3 = 0$}
\label{tab:datasetDy3=0}
\end{table}

\vspace*{\fill}

\collumnbreak

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
D     & $y_3$ & $y_{out}$ \\ \hline
$x_1$ & 1     & A         \\ \hline
$x_7$ & 1     & A         \\ \hline
$x_9$ & 1     & B         \\ \hline
\end{tabular}
\caption{Dataset D com $y_3 = 1$}
\label{tab:datasetDy3=1}
\end{table}


\vspace*{\fill}
\collumnbreak

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
D     & $y_3$ & $y_{out}$ \\ \hline
$x_4$ & 2     & C         \\ \hline
$x_6$ & 2     & A         \\ \hline
$x_8$ & 2     & A         \\ \hline
$x_{11}$ & 2     & C         \\ \hline
$x_{12}$ & 2     & C         \\ \hline
\end{tabular}
\caption{Dataset D com $y_3 = 2$}
\label{tab:datasetDy3=2}
\end{table}
\end{multicols}

\[ H(y_{out}|y_3 = 0) = - \left( \frac{3}{4} \log_2 \left( \frac{3}{4} \right) + \frac{1}{4} \log_2 \left( \frac{1}{4} \right) \right) = 0.81128 \]

\[ H(y_{out}|y_3 = 1) = - \left( \frac{2}{3} \log_2 \left( \frac{2}{3} \right) + \frac{1}{3} \log_2 \left( \frac{1}{3} \right) \right) = 0.9183 \]

\[ H(y_{out}|y_3 = 2) = - \left( \frac{2}{5} \log_2 \left( \frac{2}{5} \right) + \frac{3}{5} \log_2 \left( \frac{3}{5} \right) \right) = 0.97095 \]

Assim, podemos calcular a entropia de $y_{out}$ condicionada a $y_3$:

\[ H(y_{out}|y_3) = \frac{4}{12} H(y_{out}|y_3 = 0) + \frac{3}{12} H(y_{out}|y_3 = 1) + \frac{5}{12} H(y_{out}|y_3 = 2) = \]
\[ = \frac{4}{12} \times 0.81128 + \frac{3}{12} \times 0.9183 + \frac{5}{12} \times 0.97095 = 0.90456 \]

Por fim, podemos calcular o Information Gain:

\[ IG(y_{out}|y_3) = H(y_{out}) - H(y_{out}|y_3) = 1.58496 - 0.90456 = 0.6804 \]

\subsection{Information Gain de $y_{out}$ condicionada a $y_4$}

\[ IG(y_{out}|y_4) = H(y_{out}) - H(y_{out}|y_4) \]

\[ H(y_{out}|y_4) = \sum_{i=0}^{2} p_{y_4 = i} H(y_{out}|y_4 = i) \]

Tabela dividida em 3 sub-tabelas, cada uma com os dados que verificam $y_4 = 0$, $y_4 = 1$ e $y_4 = 2$, respetivamente:

\begin{multicols}{3}
\setlength{\columnseprule}{0pt}

\vspace*{\fill}
\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline  
D     & $y_4$ & $y_{out}$ \\ \hline
$x_1$ & 0     & A         \\ \hline
$x_2$ & 0     & B         \\ \hline
$x_3$ & 0     & B         \\ \hline
$x_8$ & 0     & A         \\ \hline
$x_{12}$ & 0     & C         \\ \hline
\end{tabular}
\caption{Dataset D com $y_4 = 0$}
\label{tab:datasetDy4=0}
\end{table}

\vspace*{\fill}
\collumnbreak

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
D     & $y_4$ & $y_{out}$ \\ \hline
$x_4$ & 1     & C         \\ \hline
$x_6$ & 1     & A         \\ \hline
$x_9$ & 1     & B         \\ \hline
$x_{10}$ & 1     & B         \\ \hline
\end{tabular}
\caption{Dataset D com $y_4 = 1$}
\label{tab:datasetDy4=1}
\end{table}

\vspace*{\fill}
\collumnbreak

\begin{table}[H]
\centering
\begin{tabular}{|c|c|c|}
\hline
D     & $y_4$ & $y_{out}$ \\ \hline
$x_5$ & 2     & C         \\ \hline
$x_7$ & 2     & A         \\ \hline
$x_{11}$ & 2     & C         \\ \hline
\end{tabular}
\caption{Dataset D com $y_4 = 2$}
\label{tab:datasetDy4=2}
\end{table}

\end{multicols}

\[ H(y_{out}|y_4 = 0) = - \left( \frac{2}{5} \log_2 \left( \frac{2}{5} \right) + \frac{2}{5} \log_2 \left( \frac{2}{5} \right) + \frac{1}{5} \log_2 \left( \frac{1}{5} \right) \right) = 1.52193 \]

\[ H(y_{out}|y_4 = 1) = - \left( \frac{1}{4} \log_2 \left( \frac{1}{4} \right) + \frac{2}{4} \log_2 \left( \frac{2}{4} \right) + \frac{1}{4} \log_2 \left( \frac{1}{4} \right) \right) = 1.5 \]

\[ H(y_{out}|y_4 = 2) = - \left( \frac{1}{3} \log_2 \left( \frac{1}{3} \right) + \frac{2}{3} \log_2 \left( \frac{2}{3} \right) \right) = 0.9183 \]

Assim, podemos calcular a entropia de $y_{out}$ condicionada a $y_4$:

\[ H(y_{out}|y_4) = \frac{5}{12} H(y_{out}|y_4 = 0) + \frac{4}{12} H(y_{out}|y_4 = 1) + \frac{3}{12} H(y_{out}|y_4 = 2) = \]
\[ = \frac{5}{12} \times 1.52193 + \frac{4}{12} \times 1.5 + \frac{3}{12} \times 0.9183 = 1.3637 \]

Por fim, podemos calcular o Information Gain:

\[ IG(y_{out}|y_4) = H(y_{out}) - H(y_{out}|y_4) = 1.58496 - 1.3637 = 0.22126 \]

\subsection{Construção da árvore de decisão}

Ordenando os IG por ordem decrescente obtemos:

\[ IG(y_{out}|y_3) > IG(y_{out}|y_2) > IG(y_{out}|y_4) \]

Assim, o nó com $y_1>0.4$ corresponde a $y_3$. 
A variável $y_3$ tem 3 possíveis valores, pelo que a árvore de decisão terá 3 ramos: como estamos condicionados a $y_1>0.4$, temos as seguintes ocorrências em cada ramo:

\[ \#(y_3=0|y_1>0.4) = 1 \]
\[ \#(y_3=1|y_1>0.4) = 2 \]
\[ \#(y_3=2|y_1>0.4) = 4 \]

Assim, apenas o nó $y_3=2$ tem pelo menos 4 ocorrências, logo, é o único que é expandido para a variável $y_2$.

Nenhum dos ramos da variável $y_2$ tem pelo menos 4 ocorrências, pelo que nenhum deles é expandido para a variável $y_4$ e termina a árvore de decisão.

A árvore de decisão final é:

\begin{forest}
  for tree={
    edge={->},
    parent anchor=south,
    child anchor=north,
    align=center,
    base=top,
    draw,
    font=\sffamily,
    s sep=5mm, % Vertical separation between siblings
    l sep=10mm, % Horizontal separation between parent and child nodes
  }
  [y_1, circle
    [$y_2$, edge label={node[midway,above]{ $y_1 \leq 0.4$}}, circle
      [C, diamond]
      [A, diamond]
      [B, diamond]
    ]
    [$y_3$, edge label={node[midway,above]{ $y_1 > 0.4$}}, circle
      [B, diamond, label= {$y_3 = 0$}]
      [A, diamond, edge label={node[midway,above]{ $y_3 = 1$}}]
      [$y_2$, edge label={node[midway,above]{ $y_3 = 2$}}, circle
        [A, diamond]
        [A, diamond]
        [C, diamond]
      ]
    ]
  ]
\end{forest}




\end{document}